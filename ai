#!/usr/bin/env bash

# MIT License

# Copyright (c) 2024 Stefan Kapusniak (one-lithe-rune)

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software
# and associated documentation files (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute,
# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software
# is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or
# substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

connection_file="$HOME/.ai-connection"

# https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
function format_llama3chat {
  local request_data=$(echo "$session_data" | jq '[.[] | {role, content:("<|start_header_id|>"+.role+"<|end_header_id|>\n\n"+.content+"<|eot_id|>") }]')
  local request_data=$(echo "$request_data" | jq --argjson props "$request_props" '$props + {messages:(. + [{role:"assistant",content:"<|start_header_id|>assistant<|end_header_id|>\n\n"}])}')
  echo "$request_data"
}

# https://github.com/tatsu-lab/stanford_alpaca
function format_alpaca {
  local request_data=$(echo "$session_data" | jq '[.[] | {role, content:((if .role=="system" then "" elif .role=="user" then "### Instruction:\n" elif .role=="input" then "### Input:\n" else "### Response:\n" end)+.content+"\n\n") }]')
  if [ "$start_new" = true ]; then
    local request_data=$(echo "$request_data" | jq --argjson props "$request_props" '$props + {messages:(. + [{role:"assistant",content:"### Response:\n"}])}')
  else
    local request_data=$(echo "$request_data" | jq --argjson props "$request_props" '$props + {messages: .} ')
  fi
  echo "$request_data"
}

# https://docs.cohere.com/docs/prompting-command-r
function format_command_r {
  exit 1  # TODO
}

function connection_details {
  echo "endpoint=$endpoint"
  echo "prompt_format=$prompt_format"
  echo "session_file=$session_file"
  echo "response_tput=$response_tput"
  echo "role=$role"
  echo "max_tokens=$max_tokens"
  echo "api_key_var=$api_key_var"
  echo "model=$model"
  echo "samplers=$samplers"
  echo "sampler_settings=$sampler_settings"
  echo ""
}

function write_connection_file {
  rm $connection_file
  echo "$(connection_details)" > "$connection_file"
  echo "" >> "$connection_file"
}

# connection defaults. Will be overriden by the connection file
endpoint="http://localhost:5001/v1/chat/completions"
prompt_format="llama3chat"
session_file="$HOME/.ai-default-session.json"
response_tput="setaf 4 bold"
prompt_tput="setaf 7 bold"
role="user"
max_tokens=1024
api_key_var="OPENAI_API_KEY"
model="gpt-4o"
samplers="temperature dynatemp_range dynatemp_exponent \
smoothing_factor top_k top_p min_p presence_penalty top_a \
tfs rep_pen rep_pen_range rep_pen_slope sampler_order"
sampler_settings=""

# write an connection file with the defaults if we don't have one
if [ ! -f "$connection_file" ]; then
  write_connection_file
else
# otherwise read the connection details in
  source $connection_file
fi

[ "$1" = "-h" -o "$1" = "--help" ] && echo "
  ai - send a prompt to an llm server from the command line

  Usage: ai <prompt>
         ai \"<prompt>\"
         ai [option] <parameter>

  ai is a bash script for prompting an llm server from the commandline.

  Since ai is a commandline script the prompt can be piped in from another
  tool and/or the ai response can be redirected to a file or piped into
  another command.

  When sending the prompt to the server it includes the history of the
  current conversation. To allow this it maintains a session file of prompts
  and responses. Which file is used for this can be switched with the
  --session-file option, allowing you to have multiple conversation threads.

  Command options:
    Currently only one option at a time is supported.

    Session:
      --session-file [FILENAME]      specify FILENAME as the file to
                                     use as the current conversation log
      --clear                        clear the current conversation log
                                     of user and assistant entries
      --clear-all                    clear the current conversation log of
                                     user, assistant and system entries
      --cat                          outputs the whole of the current
                                     conversation log to stdout

    Prompting:
      [PROMPT TEXT]                  sends PROMPT TEXT to the server
      --system [PROMPT TEXT]         sends PROMPT TEXT to the server as a
                                     system prompt
      --assistant [PROMPT TEXT]      sends PROMPT TEXT to the server as
                                     if it were a prior server response

      Note PROMPT TEXT will be read from a pipe in preference, if text is
      piped into the script.

    Server Connection:
      --load-connection [FILENAME]   load connection settings from FILENAME
      --save-connection [FILENAME]   save connection settings to FILENAME
      --endpoint [URL]               sets the endpoint URL to send requests
      --model [MODELNAME]            llm model to request the server use when
                                     processing the prompts
      --prompt-format [FORMAT]       prompt format to use for the model
                                     can be 'alpaca','llama3chat','oaichat'
      --api-key-var [ENVAR_NAME]     use ENVAR_NAME as the environment
                                     variable from which to read the api key
                                     to use for server requests
      --sampler-settings [JSONFILE]  include sampler settings specified in
                                     JSONFILE when sending server requests

      Currently only OpenAI compatible connection endpoints are supported.

      For these options if no parameter is given for the option, the current
      value will be displayed. Consult your llm server's documentation for
      whether settings such as --model and --api-key-var are required.

" && exit 0

# if we have a command that doesn't send to the ai server then handle it here and exit
if [ "--clear-all" == "$1" ]; then
  rm -f $session_file

  echo ""; echo "ai session history $session_file cleared"; echo
  exit 0
elif [ "--clear" == "$1" ]; then
  echo $(jq '[.[] | select(.role=="system")]' $session_file) > $session_file

  echo ""; echo "ai session history $session_file cleared of non-system entries"; echo
  exit 0
elif [ "--role" == "$1" ]; then
  if [ ! -z "$2" ]; then
    role="$2";
  fi
  write_connection_file

  echo ""; echo "using '$role' role when prompting the ai server"; echo
  exit 0
elif [ "--session-file" == "$1" ]; then
  if [ ! -z "$2" ]; then session_file=$(realpath $2); fi
  write_connection_file

  echo
  if [ -f "$session_file" ]; then
    echo "using existing ai session history file $session_file"
  else
    echo "using new ai session history file $session_file"
  fi
  echo
  exit 0
elif [ "--cat" == "$1" ]; then
  color_a=$(tput $response_tput)
  color_u=$(tput $prompt_tput)
  color_n=$(tput sgr0)

  cat "$session_file" | \
    jq -M --arg color_u $color_u --arg color_a $color_a --arg color_n $color_n \
      '.[] | (if .role=="assistant" then "\($color_a)\(.content)\($color_n)" else "\($color_u)\(.content)\($color_n)" end)' | \
    while read -r content; do printf "\n$content\n"; done;
  exit 0
elif [ "--prompt-format" == "$1" ]; then
  if [ ! -z "$2" ]; then prompt_format="$2"; fi
  write_connection_file

  echo ""; echo "using $prompt_format ai prompt format"; echo
  exit 0
elif [ "--model"  == "$1" ]; then
  if [ ! -z "$2" ]; then model="$2"; fi
  write_connection_file

  echo ""; echo "requesting '$model' as the model used by the ai server"; echo
  exit 0
elif [ "--endpoint" == "$1" ]; then
  if [ ! -z "$2" ]; then endpoint="$2"; fi
  write_connection_file

  echo ""; echo "using '$endpoint' as the ai server endpoint"; echo
  exit 0
elif [ "--api-key-var" == "$1" ]; then
  if [ ! -z "$2" ]; then api_key_var="$2"; fi
  write_connection_file

  echo ""; echo "api key to send to the ai server will be read from the '$api_key_var' environment variable";
  if [ -n "${!api_key_var}" ]; then
    echo "Since '$api_key_var' is currently unset, you will need to set its value if the endpoint needs an api key";
  else
    echo "Since '$api_key_var' has a value set, that value will be sent as the api key to the endpoint";
  fi
  echo
  exit 0
elif [ "--load-connection" == "$1" ]; then
  if [ -f $(realpath $2) ]; then
    current_session_file="$session_file"
    source $(realpath $2)
    session_file="$current_session_file"
    write_connection_file

    echo ""; echo "Connection values loaded from '$(realpath $2)':"; echo
    cat "$connection_file"

    if [ -n "${!api_key_var}" ]; then
      echo "Since '$api_key_var' is currently unset, you will need to set its value if the endpoint needs an api key";
    else
      echo "Since '$api_key_var' has a value set, that value will be sent as the api key to the endpoint";
    fi
    echo
  else
    echo ""; echo "File not found: $(realpath $2)"; echo
  fi
  exit 0
elif [ "--save-connection" == "$1" ]; then
  if [ ! -z "$2" ]; then
    dest_file=$(realpath "$2")
    path_status=$?
    if [ $path_status -eq 0 ]; then
      cp "$connection_file" "$dest_file"

      echo ""; echo "Connection values saved to '$dest_file'"; echo
      exit 0
    else
      echo ""; echo "Path to filename does not exist"; echo
      exit 1
    fi
  else
    echo ""; echo "No filename supplied"; echo
    exit 1
  fi
elif [ "--sampler-settings" == "$1" ]; then
  if [ ! -z "$2" ]; then
    dest_file=$(realpath -e "$2")
    path_status=$?
    if [ $path_status -eq 0 ]; then
      sampler_settings="$dest_file"

      write_connection_file
      echo ""; echo "Sampler settings from '$dest_file' will be used for this connection"; echo
      exit 0
    else
      echo ""; echo "Sampler file '$dest_file' does not exist"; echo
      exit 1
    fi
  else
    sampler_settings=""
    write_connection_file

    echo ""; echo "No custom sampler settings will be used for this conenction"; echo
    exit 0
  fi
fi

# if we have a command that does send to the ai server, process it here
if [ "--assistant" == "$1" ]; then
  role="assistant"
  cmd_line_array=( "$@" )
  cmd_line_arg=$(echo "${cmd_line_array[@]:1}")
elif [ "--system" == "$1" ]; then
  role="system"
  cmd_line_array=( "$@" )
  cmd_line_arg=$(echo "${cmd_line_array[@]:1}")
else
  # send everything if there is no recognised command
  cmd_line_arg="$*"
fi

# if stdin in not interactive (e.g. piped in) then we want to use
# that as the content to send instead of the remaing command line
# arguments
if [ -t 0 ]; then
  new_content="$cmd_line_arg"
else
  new_content=$(cat)
fi

# read json of a previous session from a file
if [ ! -f "$session_file" ]; then
  session_data='[]'
else
  session_data=$(<$session_file)
fi

# read any sampler settings file that has been set
if [ -n "$sampler_settings" ] && [ -f "$sampler_settings" ]; then
  request_props=$(cat "$sampler_settings" | jq "{$samplers}" )
else
  request_props="{}"
fi

# add a new json object to the existing list, if we have new content
if [ -n "$new_content" ]; then
  start_new=true
  session_data=$(echo "$session_data" | jq --arg content "$new_content" --arg role "$role" '. + [{ "role": $role, "content": $content }]' )
else
  start_new=false
fi

# build any additonal JSON properties we want to send with the request
request_props=$(echo "$request_props" | jq --arg max_tokens "$max_tokens" --arg model "$model" '. + {max_tokens:($max_tokens|tonumber),"model":$model}')

# format request appropriately for the model
if [[ $prompt_format = "llama3chat" ]]
then
  request_data="$(format_llama3chat)"
elif [[ $prompt_format = "alpaca" ]]
then
  request_data="$(format_alpaca)"
else
  request_data=$(echo "$session_data" | jq --argjson props "$request_props" '$props + {messages: .}')
fi

if [ -n "${!api_key_var}" ]; then
  response=$(curl -s -X POST -H "Content-Type: application/json" -H "Authorization: Bearer ${!api_key_var}" -d "$request_data" $endpoint)
else
  response=$(curl -s -X POST -H "Content-Type: application/json" -d "$request_data" $endpoint)
fi

curl_status=$?
if [ $curl_status -ne 0 ]; then
  echo ""; echo "error connecting to $endpoint"; echo
  exit $curl_status
else
  response=$(echo "$response" | jq -r '.choices[0].message')
  response_content=$(echo "$response" | jq -r '.content')
fi

echo
tput $response_tput;  echo "$response_content"; tput sgr0
echo

# add the new content to the session
echo "$session_data" | jq --argjson new_content "$response" '. + [$new_content]' > "$session_file"

